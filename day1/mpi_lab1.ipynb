{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to MPI - Day 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Code Description\n",
    "\n",
    "### 1.1 The model problem\n",
    "The code is a numerical solver for 2D Poisson equation with Dirichlet boundary condition discretised by 2nd order central finite difference operator on a unit square domain. \n",
    "\n",
    "Consider \n",
    "\n",
    "\\begin{align*}\n",
    "-\\Delta u &= f \\; \\text{in} \\; \\Omega \\\\\n",
    " u &= g \\; \\text{on} \\; \\partial \\Omega\n",
    "\\end{align*}\n",
    "\n",
    "for $\\Omega = [0,1] \\times [0,1]$.\n",
    "\n",
    "Define a uniform partition of the domain $\\Omega$ with nodal points at which the solution of the Poisson equation is sampled. Let $h$ be the uniform distance between two nodal points then the nodal points that lie on the mesh are defined by\n",
    "\n",
    "\\begin{align*}\n",
    "x_i = i h, \\; y_j = j h\\qquad i,j = 0,\\cdots, N\n",
    "\\end{align*}\n",
    "where $N$ is a given mesh size and $i, j$ are integers along $x, y$-axis telling the location of each nodal point.  \n",
    "\n",
    "### 1.2 Discretisation\n",
    "We use the second-order central finite difference method to discretise the Laplace operator\n",
    " \\begin{align*}\n",
    "     \\big(\\Delta u)\\big)_{i,j} & = \\big(D_{xx}^2u\\big)_{i,j}+ \\big(D_{yy}^2 u\\big)_{i,j}\\\\[2ex]\n",
    "     & \\approx \\frac{u_{i+1,j}-2u_{i,j}+u_{i-1,j}}{h^2} + \\frac{u_{i,j+1}-2u_{i,j}+u_{i,j-1}}{h^2} \n",
    " \\end{align*}\n",
    "\n",
    "leading to\n",
    "\\begin{align}\n",
    "    -\\big(\\Delta u)\\big)_{i,j} = \\frac{4u_{i,j}-u_{i+1,j}-u_{i-1,j}-u_{i,j+1}-u_{i,j-1}}{h^2}=f(u_{i,j}).\n",
    "\\end{align}\n",
    "\n",
    "The above finite-difference formula can further be represented by a five-point stencil matrix built in the mesh\n",
    "\\begin{align*}S = \n",
    "    \\begin{pmatrix}\n",
    "    & -1 & \\\\\n",
    "    -1 & 4 &-1\\\\\n",
    "    & -1 &\n",
    "    \\end{pmatrix}.\n",
    "\\end{align*}\n",
    "\n",
    "Impose the Dirichlet boundary condition, on the interior nodal points the discretisation can be written as a linear equation\n",
    "\n",
    "\\begin{align*}\n",
    "Au = f, \\qquad A=\\frac{1}{h^2}\n",
    "    \\begin{pmatrix}\n",
    "S & I \\\\\n",
    "I & S & I \\\\\n",
    "& I & \\ddots & \\ddots \\\\\n",
    "& & \\ddots & \\ddots & I \\\\\n",
    "& & & I & S\n",
    "\\end{pmatrix}\n",
    "\\end{align*}\n",
    "where $A \\in \\mathbb{R}^{(N-2)^2 \\times (N-2)^2}$, $\\bm{u} \\in \\mathbb{R}^{(N-2)^2}$ and $\\bm{f} \\in \\mathbb{R}^{(N-2)^2}$.\n",
    "\n",
    "Note that with the five-point stencil, the matrix $A$ was never assembled and is nowhere in sight! \n",
    "\n",
    "### 1.3 Numerical Solvers\n",
    "\n",
    "To solve the linear system, the Jacobi iterative method is used and compared. \n",
    "\n",
    "$\\textbf{Jacobi method}$\n",
    "\n",
    "\\begin{align*}\n",
    "u^{(k+1)} = D^{-1}( f - Lu_{k} -Uu_k),\n",
    "\\end{align*}\n",
    "where $D, L, U$ are the diagonal matrix, lower triangular matrix and upper triangular matrix of $A$, respectively.\n",
    "Write into stecil,\n",
    "\\begin{align*}\n",
    "u^{(k+1)}_{ij} = (h^2 f_{ij} + u^{(k)}_{i-1,j} +u^{(k)}_{i+1,j} + u^{(k)}_{i, j-1}+u^{(k)}_{i,j+1})/4 \\qquad i,j = 1,\\cdots, N-1 \n",
    "\\end{align*} \n",
    "\n",
    "The method is implemented in file [solver.c](./solver.c) if you want a peak but remember no refactoring is needed for routines in this file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Convergence measurement \n",
    "Usually, it is impossible to measure the error of the numerical solver as it requires to know the true solution $e^{(k)}= u^{(k)} -u^\\ast$ where $u^\\ast$ is unkown. \n",
    "\n",
    "Often the measurement of the differene between appproximation is seen, i.e. $\\lim_{k\\to \\infty} d^{(k)} = u^{(k)} - u^{(k-1)} = 0$. However this cauchy sequency type of quantity has no implication of the convergence to the matrix problem. Therefore this practice should be avoided.\n",
    "\n",
    "A standard measurement is by the residual of the problem,\n",
    "$$r^{(k)} = f - Au^{(k)}$$\n",
    "as implemented in [solver.c](./solver.c), owing to the following relation\n",
    "$$Ae^{(k)} = r^{(k)} \\implies  \\|r\\|\\le \\|A\\| \\|e\\|.$$\n",
    "Implies $$\\|e^{(k)}\\| \\le \\|A^{-1}\\| \\|r^{(k)}\\|,$$\n",
    "that is $$\\lim_{k\\to \\infty} \\|r^{(k)}\\| = 0 \\implies \\lim_{k\\to \\infty}\\|e^{(k)}\\| = 0.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Parallelisation\n",
    "As disscused in the slides, the unit square domain is decomposed into to subdomains for parallelisation. Each MPI process hosts a subdomain. \n",
    "```cpp\n",
    "double (*submesh)[mesh_size] = malloc(sizeof *submesh * *ptr_rows);\n",
    "```\n",
    "where mesh_size is the number of grid points per dimension. \n",
    "\n",
    "\n",
    "The decomposition is performed vertically, resulting two rows of ghost nodes on the top and the bottom for each subdomain.\n",
    "```cpp\n",
    "/* top row of ghost nodes */\n",
    "submesh[*ptr_rows -1]\n",
    "\n",
    "/*bottom row of ghost nodes */\n",
    "submesh[0]\n",
    "```\n",
    "The mesh initialisation is written in [mesh.c](./mesh.c). Please do not edit.\n",
    "\n",
    "Note that the value that the pointer ptr_rows pointing to is different for the top slab subdomain as it may host extra number of rows. The following example demonstrates the decomposition for a domain of 27x27 distributed to 4 MPI processes. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "+-------------+  \n",
    "|MPI Process 3| -> subdomain 27 x 9 (7 interior rows + 2 ghost rows) top ghost row is used as the boundary\n",
    "+------------+ \n",
    "|MPI Process 2| -> subdomain 27 x 8 (6 interior rows + 2 ghost rows)\n",
    "+-------------+\n",
    "|MPI Process 1| -> subdomain 27 x 8 (6 interior rows + 2 ghost rows)\n",
    "+-------------+ \n",
    "|MPI Process 0| -> subdomain 27 x 8 (6 interior rows + 2 ghost rows) bottom ghost row is used as the boundary\n",
    "+-------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Blocking Communication\n",
    "\n",
    "Each subdomain needs to update its ghost rows from neighbouring processes and send the top and bottom rows of full nodes to the neighbouring processes. \n",
    "\n",
    "For a MPI process $i$ to recv the update on the top row of ghost nodes with a standard blocking communication, we use\n",
    "```cpp \n",
    "/* on MPI process i */\n",
    " MPI_Recv(submesh[*ptr_rows -1], mesh_size, MPI_DOUBLE, upper, highertag, MPI_COMM_WORLD, &status);\n",
    "```\n",
    "\n",
    "This requires the upper rank $i+1$ to coordinate with a send call\n",
    "```cpp\n",
    "/* on MPI process i+1 */\n",
    "MPI_Send(submesh[1], mesh_size, MPI_DOUBLE, lower, highertag, MPI_COMM_WORLD);\n",
    "```\n",
    "\n",
    "Because every processes will execute those two calls, they can be put together without specifying the rank. See [laplace_mpi_blocking.c](./laplace_mpi_blocking.c)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`TODO`**: in [laplace_mpi_blocking.c](./laplace_mpi_blocking.c), only top ghost row is updated, complete the same communication for the bottom ghost row. Once you are finished run the next cell. If you are stuck, peek solution at [soln_laplace_mpi_blocking.c](./solutions/laplace_mpi_blocking.c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f *.o laplace_mpi_blocking laplace_mpi_nonblocking laplace_mpi_persistent\n",
      "mpicc -g -Wall -O3 -lm -o laplace_mpi_blocking laplace_mpi_blocking.c mesh.c solver.c\n",
      "\u001b[01m\u001b[Klaplace_mpi_blocking.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kmain\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Klaplace_mpi_blocking.c:226:12:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kw_count\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-variable\u0007-Wunused-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "  226 | int count, \u001b[01;35m\u001b[Kw_count\u001b[m\u001b[K ;\n",
      "      |            \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
      "Compilation Successful!\n",
      "[WARNING] yaksa: 1 leaked handle pool objects\n",
      "[WARNING] yaksa: 1 leaked handle pool objects\n",
      "[WARNING] yaksa: 1 leaked handle pool objects\n",
      "[WARNING] yaksa: 1 leaked handle pool objects\n",
      "[WARNING] yaksa: 1 leaked handle pool objects\n",
      "[WARNING] yaksa: 1 leaked handle pool objects\n",
      "[WARNING] yaksa: 1 leaked handle pool objects\n",
      "[WARNING] yaksa: 1 leaked handle pool objects\n",
      "Residual  0.075310 after iter 482 \n",
      "Residual  0.075272 after iter 483 \n",
      "Residual  0.075234 after iter 484 \n",
      "Residual  0.075196 after iter 485 \n",
      "Residual  0.075158 after iter 486 \n",
      "Residual  0.075120 after iter 487 \n",
      "Residual  0.075082 after iter 488 \n",
      "Residual  0.075045 after iter 489 \n",
      "Residual  0.075007 after iter 490 \n",
      "Residual  0.074969 after iter 491 \n",
      "Residual  0.074931 after iter 492 \n",
      "Residual  0.074894 after iter 493 \n",
      "Residual  0.074856 after iter 494 \n",
      "Residual  0.074818 after iter 495 \n",
      "Residual  0.074780 after iter 496 \n",
      "Residual  0.074743 after iter 497 \n",
      "Residual  0.074705 after iter 498 \n",
      "Residual  0.074668 after iter 499 \n",
      "Residual  0.074630 after iter 500 \n",
      "Residual1  0.077502\n"
     ]
    }
   ],
   "source": [
    "!make clean && make blocking && echo \"Compilation Successful!\" && mpiexec -np 8 ./laplace_mpi_blocking 100 500 Jacobi > convergence_blocking.txt\n",
    "!tail -20 convergence_blocking.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonblocking Communication\n",
    "During the communication between MPI processes, the iterative method can perform on interior grid nodes (except top and bottom full nodes) concurrently. To this end, a `Jacobi_int` routine is used and separated from `Jacobi_top` and `Jacobi_bottom` (see [solver.c](./solver.c)).\n",
    "\n",
    "Hence, the communication starts from sending and receiving data for the top and bottom rows, and is followed by the `Jacobi_int` routine. Before applying `Jacobi_top` and `Jacobi_bottom` respectively, one needs to ensure the corresponding communication call is completed. The communications use `MPI_Request`\n",
    "```cpp\n",
    " MPI_Request top_bnd_requests[2],  bottom_bnd_requests[2];\n",
    "```\n",
    "to identify the communications. \n",
    "\n",
    "In the code [lapalce_mpi_nonblocking.c](./laplace_mpi_nonblocking.c), it is written based on the \"first in first served\" concept - whichever request is completed, it gets to proceed with the Jacobi method. \n",
    "```cpp\n",
    "    /* Test on either the top or bottom layer */\n",
    "    if ( (MPI_Testall(2, top_bnd_requests, &top_flag, top_bnd_status) > 0) || (MPI_Testall(2, bottom_bnd_requests, &bottom_flag, bottom_bnd_status) > 0))\n",
    "    {\n",
    "       MPI_Abort(MPI_COMM_WORLD, 1);\n",
    "    }\n",
    "\n",
    "    /* if the top layer is ready */\n",
    "    if (top_flag){\n",
    "        /* perform jacobi on the top bnd */\n",
    "        Jacobi_top(ptr_rows, mesh_size, &submesh[0][0], &submesh_new[0][0], &subrhs[0][0], space);\n",
    "\n",
    "    /* if the the bottom layer is ready */\n",
    "    if (bottom_flag){\n",
    "    /* perform jacobi on the bottom bnd */\n",
    "    Jacobi_bottom(ptr_rows, mesh_size, &submesh[0][0], &submesh_new[0][0], &subrhs[0][0], space);\n",
    "    }\n",
    "    \n",
    "    /* if the bottom layer is yet ready */\n",
    "    else{\n",
    "        /* wait on the bottom layer */\n",
    "        MPI_Waitall(2, bottom_bnd_requests, bottom_bnd_status);\n",
    "        Jacobi_bottom(ptr_rows, mesh_size, &submesh[0][0], &submesh_new[0][0], &subrhs[0][0], space);        \n",
    "        }\n",
    "    }\n",
    "    /* if the top layer is yet ready but the buttom is */\n",
    "    else if (bottom_flag){\n",
    "        /* perform jacobi bottom ready */\n",
    "        Jacobi_bottom(ptr_rows, mesh_size, &submesh[0][0], &submesh_new[0][0], &subrhs[0][0], space);\n",
    "        /* wait on the top layer */\n",
    "        MPI_Waitall(2, top_bnd_requests, top_bnd_status);\n",
    "        Jacobi_top(ptr_rows, mesh_size, &submesh[0][0], &submesh_new[0][0], &subrhs[0][0], space);\n",
    "        }\n",
    "    /* if neither of the top and bottom is ready, then wait on both */\n",
    "    else {\n",
    "        MPI_Waitall(2, bottom_bnd_requests, bottom_bnd_status);\n",
    "        Jacobi_bottom(ptr_rows, mesh_size, &submesh[0][0], &submesh_new[0][0], &subrhs[0][0], space);\n",
    "        MPI_Waitall(2, top_bnd_requests, top_bnd_status);\n",
    "        Jacobi_top(ptr_rows, mesh_size, &submesh[0][0], &submesh_new[0][0], &subrhs[0][0], space);\n",
    "    }\n",
    "```\n",
    "However, this may overcomplicated the program. \n",
    "\n",
    "**`TODO`**  Change the code in [laplace_mpi_nonblocking.c](./laplace_mpi_nonblocking.c) to a simplifed version that binds both top and bottom communications into one batch of requests. Once you are finished run the next cell. If you are stuck, peek solution at [soln_laplace_mpi_nonblocking.c](./solutions/laplace_mpi_nonblocking.c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f *.o laplace_mpi_blocking laplace_mpi_nonblocking laplace_mpi_persistent\n",
      "mpicc -g -Wall -O3 -lm -o laplace_mpi_nonblocking laplace_mpi_nonblocking.c mesh.c solver.c\n",
      "\u001b[01m\u001b[Klaplace_mpi_nonblocking.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kmain\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Klaplace_mpi_nonblocking.c:144:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kindex\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-variable\u0007-Wunused-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "  144 | int \u001b[01;35m\u001b[Kindex\u001b[m\u001b[K, top_flag, bottom_flag;\n",
      "      |     \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
      "Compilation Successful!\n",
      "Residual  0.075310 after iter 482 \n",
      "Residual  0.075272 after iter 483 \n",
      "Residual  0.075234 after iter 484 \n",
      "Residual  0.075196 after iter 485 \n",
      "Residual  0.075158 after iter 486 \n",
      "Residual  0.075120 after iter 487 \n",
      "Residual  0.075082 after iter 488 \n",
      "Residual  0.075045 after iter 489 \n",
      "Residual  0.075007 after iter 490 \n",
      "Residual  0.074969 after iter 491 \n",
      "Residual  0.074931 after iter 492 \n",
      "Residual  0.074894 after iter 493 \n",
      "Residual  0.074856 after iter 494 \n",
      "Residual  0.074818 after iter 495 \n",
      "Residual  0.074780 after iter 496 \n",
      "Residual  0.074743 after iter 497 \n",
      "Residual  0.074705 after iter 498 \n",
      "Residual  0.074668 after iter 499 \n",
      "Residual  0.074630 after iter 500 \n",
      "Residual1  0.077502\n"
     ]
    }
   ],
   "source": [
    "!make clean && make nonblocking && echo \"Compilation Successful!\" && mpiexec -np 8 ./laplace_mpi_nonblocking 100 500 Jacobi > convergence_nonblocking.txt\n",
    "!tail -20 convergence_nonblocking.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Persistent Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be quite familiar with our code, and may notice that the same communication is executed over and over again only with different data buffer within the iteration loops. Recall the four stages of `MPI Operation` mentioned in the slides. The `initialisation stage` hands over the argument list, which stays unchanged in our program. The persistent communication binds the argument list to the communication request once and repeatedly uses it in the subsequent communication calls.  \n",
    "\n",
    "**`TODO`** Refactor the code in [laplace_mpi_persistent.c](./laplace_mpi_persistent.c), which is using previous nonblocking routines at the current status, to a persistent communication. Once you are finished run the next cell. If you are stuck, peek the solution at [soln_laplace_persistent.c](./solutions/laplace_mpi_persistent.c).\n",
    "\n",
    "Essential MPI functions needed: `MPI_Recv_init`, `MPI_Send_int`, `MPI_Startall`, `MPI_Waitall`, `MPI_Request_free`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f *.o laplace_mpi_blocking laplace_mpi_nonblocking laplace_mpi_persistent\n",
      "mpicc -g -Wall -O3 -lm -o laplace_mpi_persistent laplace_mpi_persistent.c mesh.c solver.c\n",
      "\u001b[01m\u001b[Klaplace_mpi_persistent.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kmain\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Klaplace_mpi_persistent.c:159:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kindex\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-variable\u0007-Wunused-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "  159 | int \u001b[01;35m\u001b[Kindex\u001b[m\u001b[K, top_flag, bottom_flag;\n",
      "      |     \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
      "Compilation Successful!\n",
      "[WARNING] yaksa: 1 leaked handle pool objects\n",
      "[WARNING] yaksa: 1 leaked handle pool objects\n",
      "[WARNING] yaksa: 1 leaked handle pool objects\n",
      "[WARNING] yaksa: 1 leaked handle pool objects\n",
      "[WARNING] yaksa: 1 leaked handle pool objects\n",
      "[WARNING] yaksa: 1 leaked handle pool objects\n",
      "[WARNING] yaksa: 1 leaked handle pool objects\n",
      "[WARNING] yaksa: 1 leaked handle pool objects\n",
      "Residual  0.075272 after iter 483 \n",
      "Residual  0.075234 after iter 484 \n",
      "Residual  0.075196 after iter 485 \n",
      "Residual  0.075158 after iter 486 \n",
      "Residual  0.075120 after iter 487 \n",
      "Residual  0.075082 after iter 488 \n",
      "Residual  0.075045 after iter 489 \n",
      "Residual  0.075007 after iter 490 \n",
      "Residual  0.074969 after iter 491 \n",
      "Residual  0.074931 after iter 492 \n",
      "Residual  0.074894 after iter 493 \n",
      "Residual  0.074856 after iter 494 \n",
      "Residual  0.074818 after iter 495 \n",
      "Residual  0.074780 after iter 496 \n",
      "Residual  0.074743 after iter 497 \n",
      "Residual  0.074705 after iter 498 \n",
      "Residual  0.074668 after iter 499 \n",
      "Residual  0.074630 after iter 500 \n",
      "Final residual  0.077502\n",
      "size 8size 8size 8size 8size 8size 8size 8size 8"
     ]
    }
   ],
   "source": [
    "!cd ./solutions && make clean && make persistent && echo \"Compilation Successful!\" && mpiexec -np 8 ./laplace_mpi_persistent 100 500 Jacobi > convergence_persistent.txt\n",
    "!cd ./solutions && tail -20 convergence_persistent.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
